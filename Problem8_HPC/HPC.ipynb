{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgorBaratta/FEniCSxCourse/blob/ICMC23/Problem8_HPC/HPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no6dI0BMtdaN"
      },
      "source": [
        "# DOLFINx in Parallel with MPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX9ypdWitdaQ"
      },
      "source": [
        "To run the programs in this section, first you will need to install the dolfinx and the mpi4py library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yINZsUrStdaR"
      },
      "outputs": [],
      "source": [
        "!wget \"https://fem-on-colab.github.io/releases/fenicsx-install-real.sh\" -O \"/tmp/fenicsx-install.sh\" && bash \"/tmp/fenicsx-install.sh\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2st97E6stdaU"
      },
      "source": [
        "## First parallel program\n",
        "\n",
        "For a quick introduction of distributed parallel computing using MPI please follow  these tutorials: [introduction-to-mpi](https://www.codingame.com/playgrounds/349/introduction-to-mpi/introduction-to-distributed-computing) or [mpitutorial](https://mpitutorial.com/).\n",
        "\n",
        "In this hands-on tutorial we only discuss the minimum for running simple codes in DOLFINx and understanding concepts that you may have already seen in the previous parts of the tutorial such as `MPI.COMM_WORLD`, `comm.size`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_iWLY3jtdaU"
      },
      "outputs": [],
      "source": [
        "%%writefile hello-world.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "print(f\"Hello world from rank {comm.rank} of {comm.size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIpDhcgftdaV"
      },
      "source": [
        "A communicator is a collection of MPI processes, and the default communicator is `MPI.COMM_WORLD`. `MPI.COMM_WORLD` basically groups all the processes when the program started. The number of processes in a communicator does not change once it is created. That number is called the `size` of the communicator. The `rank` of a process in a communicator is a unique id  from $0$ to $N-1$.\n",
        "\n",
        "Next let's see how we can use the mpirun program to execute the above python code using 4 processes. The value after `-np` is the number of processes to use when running script in parallel. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGRDhL5VtdaW"
      },
      "outputs": [],
      "source": [
        "! mpirun --allow-run-as-root --oversubscribe -np 8 python hello-world.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: MPI codes are not guaranteed to complete in any specific order."
      ],
      "metadata": {
        "id": "fEY2GegeJYH8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGpuCWwqtdaW"
      },
      "source": [
        "## Point-to-Point communication\n",
        "\n",
        "In MPI processes (and hence their memory) are totally independent. Information between proces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9tL-xSRtdaX"
      },
      "outputs": [],
      "source": [
        "%%writefile p2p.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "assert comm.size == 2\n",
        "rank = comm.rank\n",
        "\n",
        "if rank == 0:\n",
        "    data = numpy.arange(10, dtype=int)\n",
        "    comm.Send(data, dest=1)\n",
        "    print(f\"Process 0 sent {data} to process 1\")\n",
        "elif rank == 1:\n",
        "    data = numpy.zeros(10, dtype=int)\n",
        "    comm.Recv(data, source=0)\n",
        "    print(f\"Process 1 received {data} from process 0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJGoMbK-tdaY"
      },
      "outputs": [],
      "source": [
        "!mpirun -n 5 --allow-run-as-root --oversubscribe python3 p2p.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Key points:\n",
        " - MPI uses the notion of a rank to distinguish processes.\n",
        " - Send and Recv are the fundumental primitives.\n",
        " - Sending a message from one process to another is known as point-to-point communication."
      ],
      "metadata": {
        "id": "LTQ2JZDjLpq5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AX7M36ctdaZ"
      },
      "source": [
        "**Exercice**: Implement a simple communication ring.\n",
        "Each process `i` sends data to its neighbor `i+1` except for the last process in the communicator which sends data to process 0.\n",
        "\n",
        "Template:\n",
        "```python3\n",
        "from mpi4py import MPI\n",
        "import numpy\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "rank = comm.rank\n",
        "\n",
        "sent_to = rank+1 if rank<size-1 else 0\n",
        "receive_from = rank-1 if rank>0 else size-1\n",
        "\n",
        "send_data = numpy.array([rank], dtype=int)\n",
        "recv_data = numpy.zeros_like(send_data)\n",
        "\n",
        "comm.Send(TODO)\n",
        "comm.Recv(TODO)\n",
        "\n",
        "print(f\"Process {rank} received {recv_data} from process 0\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a mesh in parallel\n",
        "DOLFINx abstracts most of the difficult aspects of distributing the finite element problem across the MPI communicator away from the user. \n",
        "Let's have a look at how to create and write a mesh in parallel: "
      ],
      "metadata": {
        "id": "DUDrPpVAvIew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mesh.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "# Global communicator \n",
        "comm = MPI.COMM_WORLD\n",
        "Nprocs = comm.size\n",
        "\n",
        "# Create the mesh and distribute across Nprocs\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 20, 20)\n",
        "\n",
        "# Create a cell function to visualize the mesh distribution\n",
        "DG = dolfinx.fem.FunctionSpace(mesh, (\"DG\", 0))\n",
        "cell_value = dolfinx.fem.Function(DG)\n",
        "cell_value.x.array[:] = comm.rank\n",
        "\n",
        "# Save mesh and \n",
        "with XDMFFile(comm, f\"mesh_{comm.size}.xdmf\", \"w\") as xdmf:\n",
        "    xdmf.write_mesh(mesh)\n",
        "    xdmf.write_function(cell_value)"
      ],
      "metadata": {
        "id": "inBTmqnQwCxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np = 4\n",
        "! mpirun -np $np --allow-run-as-root --oversubscribe python3 mesh.py\n",
        "\n",
        "from google.colab import files\n",
        "files.download(f\"mesh_{np}.xdmf\")\n",
        "files.download(f\"mesh_{np}.h5\")"
      ],
      "metadata": {
        "id": "nRIAhTfZxI9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try changing the number of processes $n_p$ and visualizing the mesh."
      ],
      "metadata": {
        "id": "1_jGbGP70ogM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing data in parallel: the IndexMap\n",
        "\n",
        "The IndexMap represents the distribution index arrays across processes. \n",
        "An index array is a contiguous collection of N indices `[0, 1, . . ., N-1]` that are distributed across M processes. On a given process, the IndexMap stores a portion of the index set using local indices `[0, 1, . . . , n-1]`, and a map from the local indices to a unique global index.\n",
        "\n",
        "Let's have a look at an example. \n",
        "How many cells are owned by a given process?"
      ],
      "metadata": {
        "id": "r2IAK2TC1Jpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile index_map.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 20, 20)\n",
        "tdim = mesh.topology.dim\n",
        "idx_map = mesh.topology.index_map(tdim)\n",
        "\n",
        "N = idx_map.size_global\n",
        "n = idx_map.size_local\n",
        "print(f\"Process {comm.rank} owns {n} cells of {N}\")\n",
        "\n",
        "comm.Barrier()\n",
        "print(\"\")\n",
        "comm.Barrier()\n",
        "\n",
        "vmap = mesh.topology.index_map(0)\n",
        "print(f\"Process {comm.rank} owns {vmap.size_local} vertices of {vmap.size_global}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a5WPwb2w1-RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 2 --allow-run-as-root --oversubscribe python3 index_map.py"
      ],
      "metadata": {
        "id": "zhEwpNJ73BXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function and FunctionSpaces\n",
        "\n",
        "When we define a Function Space on a distributed mesh, each rank constructs a local part of the space on it’s part of the mesh. \n",
        "The distribution of a vector or function across MPI processes is also managed by an IndexMap and it follows the distribution of the mesh. \n",
        "\n",
        "Consider a continuous first-order Lagrange space over a mesh. The degrees of freedom of this space are associated with the vertices of the mesh.\n",
        "\n",
        "A function in DOLFINx contains memory (an array) in which the expansion coefficients ($u_i$) of the finite element basis ($\\phi_i$) can be stored:\n",
        "\n",
        "$$\n",
        "u_h = \\sum_{i = 0}^{N-1} \\phi_i u_i\n",
        "$$\n",
        "\n",
        "\n",
        "For instance, in the figure below we show an arbitrary vector of coefficients of size 15 distributed across 3 different processes.\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/IgorBaratta/FEniCSxCourse/main/Problem8_HPC/distribute.png' width=\"600\" height=\"800\"/>\n",
        "<figcaption>Layout of a parallel vector of size 15 distributed to 3 processes. From left to right: Sequential vector, distributed vector, distributed vector with ghosting.\n",
        "</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "36puB8S_3XLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile distributed_function.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 20, 20)\n",
        "V = dolfinx.fem.FunctionSpace(mesh, (\"Lagrange\", 1))\n",
        "u = dolfinx.fem.Function(V)\n",
        "\n",
        "map = V.dofmap.index_map\n",
        "\n",
        "print(f\"Process {comm.rank} owns {map.size_local} dofs of {map.size_global}\")\n",
        "\n",
        "comm.Barrier()\n",
        "print(\"\")\n",
        "comm.Barrier()\n",
        "\n",
        "print(f\"Local size {u.x.array.size} in process {comm.rank}\")\n",
        "\n",
        "# The size of local vector includes owned dofs and ghosts\n",
        "assert u.x.array.size ==  map.size_local + map.num_ghosts"
      ],
      "metadata": {
        "id": "JWOg3hGZ4FRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 4 --allow-run-as-root --oversubscribe python3 distributed_function.py"
      ],
      "metadata": {
        "id": "Xmk0AanD6RXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**:\n",
        "\n",
        "Let $N$ be the global size of a function (total number of degrees of freedom distributed across all processes), and $N_i$ the local size on process $i$ (u.x.array.size) of $P$ processes. Then:\n",
        "\n",
        "$$\n",
        "N \\leq \\sum_{i=0}^{P-1} N_i\n",
        "$$\n",
        "\n",
        "Now, let $N_i^o$ be the number owned `dofs` on process $i$, then\n",
        "$$\n",
        "N = \\sum_{i=0}^{P-1} N_i^o\n",
        "$$\n",
        "\n",
        "And\n",
        "$$\n",
        "N_i = N_i^o + N_i^{g}\n",
        "$$"
      ],
      "metadata": {
        "id": "qmpr3WeWSK0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scattering values\n",
        "\n",
        "Let’s say we want to change the expansion coefficient $u_i$ (local numbering) each processes to have a value equal to the MPI rank + 1 of the owning process. For consistency we need this change to be reflected in two places:\n",
        "\n",
        " - In the memory of the process that owns the degree of freedom.\n",
        " - In the memory of the process (if any) that has the degree of freedom as a ghost.\n",
        "\n",
        "\n",
        "How do we do that?"
      ],
      "metadata": {
        "id": "8hK_VdvV7s-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scatter.py\n",
        "\n",
        "def print_data(comm, data):\n",
        "  for i in range(comm.size):\n",
        "      if comm.rank == i:\n",
        "        print(f\"Data on process {comm.rank}: {data}\")\n",
        "      comm.Barrier()\n",
        "\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 5, 5)\n",
        "V = dolfinx.fem.FunctionSpace(mesh, (\"Lagrange\", 1))\n",
        "u = dolfinx.fem.Function(V)\n",
        "\n",
        "u.x.array[:] = comm.rank\n",
        "\n",
        "print_data(comm, u.x.array)\n",
        "\n",
        "print()\n",
        "\n",
        "# Scatter values to sharing process\n",
        "u.x.scatter_forward()\n",
        "\n",
        "print_data(comm, u.x.array)"
      ],
      "metadata": {
        "id": "_E-23bXY8FJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 4 --allow-run-as-root --oversubscribe python3 scatter.py"
      ],
      "metadata": {
        "id": "tfqd0WdR8aic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/IgorBaratta/FEniCSxCourse/main/Problem8_HPC/update.png' width=\"600\" height=\"800\"/>\n",
        "<figcaption>Forward scattering of a vector in parallel. Communication direction is dof owner to ghost.\n",
        "</figcaption></center>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "L-IkB-uTYziA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling vectors in parallel\n",
        "\n",
        "It is possible to divide linear and bilinear forms into subdomain contributions.\n",
        "\n",
        "$$\n",
        "L(v) = \\sum_{i=0}^{P-1} L_i(v) = \\sum_{i=0}^{P-1} \\int_{\\Omega_i} f \\cdot v \\, dx\n",
        "$$\n",
        "\n",
        "When we call `dolfinx.fem.assemble_vector(L)` on a given linear form $L$ the following happens:\n",
        " - Each process computes the cell tensors $b$ for cells that it owns.\n",
        " - It assembles (adds) the result into its local array.\n",
        "\n",
        "At this point no parallel communication has taken place! The vector is in an inconsistent state, a contribution to a degree of freedom might have taken place in another process.\n",
        "\n",
        "To update the values we use:\n",
        "\n",
        "```\n",
        "x.scatter_rev(ScatterMode.add)\n",
        "```\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/IgorBaratta/FEniCSxCourse/main/Problem8_HPC/accumulate.png' width=\"600\" height=\"800\"/>\n",
        "<figcaption>Reverse scattering of a vector in parallel. Communication direction is ghost to dof owner.\n",
        "</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "Let's have a look of state of the vector before and after the scatter reverse:"
      ],
      "metadata": {
        "id": "Hd07BpTN9i7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linear_form.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "from petsc4py import PETSc\n",
        "import dolfinx\n",
        "import ufl\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 2, 2, ghost_mode=dolfinx.mesh.GhostMode.none)\n",
        "V = dolfinx.fem.FunctionSpace(mesh, (\"Lagrange\", 1))\n",
        "u = dolfinx.fem.Function(V)\n",
        "v = ufl.TestFunction(V)\n",
        "\n",
        "L = ufl.inner(5.0, v)*ufl.dx\n",
        "\n",
        "b = dolfinx.fem.assemble_vector(dolfinx.fem.form(L))\n",
        "\n",
        "if comm.rank == 0:\n",
        "    print(\"Before scattering\")\n",
        "\n",
        "for i in range(comm.size):\n",
        "    if comm.rank == i:\n",
        "      print(f\"Data on process {comm.rank}: {b.array}\")\n",
        "    comm.Barrier()\n",
        "\n",
        "b.scatter_reverse(dolfinx.la.ScatterMode.add)\n",
        "\n",
        "if comm.rank == 0:\n",
        "    print(\"\\nAfter scattering\")\n",
        "\n",
        "for i in range(comm.size):\n",
        "    if comm.rank == i:\n",
        "      print(f\"Data on process {comm.rank}: {b.array}\")\n",
        "    comm.Barrier()"
      ],
      "metadata": {
        "id": "a0bPoYG_-Wf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 2 --allow-run-as-root --oversubscribe python3 linear_form.py"
      ],
      "metadata": {
        "id": "SjESK-Ti-vUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling matrices in parallel\n",
        "\n",
        "As for the linear form, the bilinear form the contributions can be computed for each subdomain separetely. For example, the bilinear form a(⋅,⋅) associated with the Poisson’s equation can be deomposed into ai(⋅,⋅) defined by:\n",
        "\n",
        "$$\n",
        "a(u,v) = \\sum_i^{P-1} a_i(u, v) = \\sum_i^{P-1} \\int_{\\Omega_i} \\nabla u \\cdot \\nabla v~dx\n",
        "$$\n",
        "\n",
        "Each process assembles its local contribution to the global bilinear form into a sparse matrix. The number of rows in the local matrix coincides with the dimension of the local function space."
      ],
      "metadata": {
        "id": "0MXiKpiNAEuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bilinear_form.py\n",
        "from mpi4py import MPI\n",
        "from petsc4py import PETSc\n",
        "import dolfinx\n",
        "import ufl\n",
        "import numpy as np\n",
        "import scipy.sparse\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 2, 2, ghost_mode=dolfinx.mesh.GhostMode.none)\n",
        "V = dolfinx.fem.FunctionSpace(mesh, (\"Lagrange\", 1))\n",
        "u = ufl.TrialFunction(V)\n",
        "v = ufl.TestFunction(V)\n",
        "\n",
        "a = ufl.inner(ufl.grad(u), ufl.grad(v)) * ufl.dx\n",
        "a = dolfinx.fem.form(a)\n",
        "\n",
        "A = dolfinx.fem.assemble.create_matrix(a)\n",
        "dolfinx.fem.assemble_matrix(A, a)\n",
        "A.finalize()\n",
        "\n",
        "\n",
        "# Create a Scipy sparse matrix that shares data with A\n",
        "As = scipy.sparse.csr_matrix((A.data, A.indices, A.indptr))\n",
        "\n",
        "print(f\"Process {comm.rank} has {As.shape[0]} rows,\" + \\\n",
        "      f\"from which {V.dofmap.index_map.size_local} are owned.\")\n",
        "\n",
        "comm.Barrier()\n",
        "if comm.rank == 0:\n",
        "  print(f\"Global number of degrees of freedom {V.dofmap.index_map.size_global}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "dN22UMsQCERv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 2 --allow-run-as-root --oversubscribe python3 bilinear_form.py"
      ],
      "metadata": {
        "id": "NTfUBAoyDvEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Scaling\n",
        "\n",
        "High performance computing has two common notions of scalability:\n",
        "\n",
        " - Strong scaling is defined as how the solution time varies with the number of processors for a fixed total problem size.\n",
        " - Weak scaling is defined as how the solution time varies with the number of processors for a fixed problem size per processor."
      ],
      "metadata": {
        "id": "qAAUV84KAVRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inclass-work (8)\n",
        "\n",
        "In this final assignmente we will solve the 3D\n",
        "Poisson's problem with homegeneous diffusivity\n",
        "coefficient and pure Dirichlet homegeneous boundary data, i.e.,\n",
        "\n",
        "\\begin{equation}\n",
        "\\left \\{\n",
        "\\begin{array}{rcll}\n",
        "-\\nabla \\cdot ( \\mu \\nabla {u} ) & = & f &  \\mbox{in}~\\Omega \\\\\n",
        "&& \\\\\n",
        "{u} & = & 0 &  \\mbox{in}~\\partial\\Omega\n",
        "\\end{array}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "where the domain $\\Omega = [0,1]^3$. The\n",
        "variational formulation is:\n",
        "\n",
        "Find $u \\in V_0$ such that \n",
        "\n",
        "\\begin{equation}\n",
        "\\int_{\\Omega}{\\mu\\,\\nabla{u}\\cdot \\nabla{v}}\\,dx = \\int_{\\Omega} {f\\,v\\,dx}~~~\\forall v \\in V_0(\\Omega)\n",
        "\\end{equation}\n",
        "\n",
        "The figure below illustrates the computational mesh:\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://raw.githubusercontent.com/IgorBaratta/FEniCSxCourse/ICMC23/Problem8_HPC/meshim3D.png' width=\"600\" height=\"500\"/>\n",
        "<figcaption>Typical 3D tetrahedral mesh to run the\n",
        "scalability tests. The example has $51\\times 51 \\times 51$ grid points.\n",
        "</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "---\n",
        "\n",
        "Scalability study: [poisson](https://github.com/IgorBaratta/FEniCSxCourse/blob/main/Problem8_HPC/poisson.py)\n"
      ],
      "metadata": {
        "id": "3CQQex9c7trU"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}